{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome Depth Anything 3 - Interactive Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aedelon/awesome-depth-anything-3/blob/main/notebooks/da3_tutorial.ipynb)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue)](https://github.com/Aedelon/awesome-depth-anything-3)\n",
    "\n",
    "This notebook demonstrates **Depth Anything 3**, a state-of-the-art model for:\n",
    "- üåä **Monocular Depth Estimation** - Depth maps from single images\n",
    "- üì∑ **Camera Pose Estimation** - Extrinsics and intrinsics from multi-view\n",
    "- ‚òÅÔ∏è **Point Cloud Reconstruction** - 3D visualization with cameras\n",
    "- üé• **Novel View Synthesis** - 3D Gaussian Splatting (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è GPU Required\n",
    "\n",
    "Go to **Runtime ‚Üí Change runtime type ‚Üí GPU** before running!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install awesome-depth-anything-3 { display-mode: \"form\" }\n",
    "# @markdown This will install the package and its dependencies (~2-3 minutes)\n",
    "\n",
    "!pip install -q awesome-depth-anything-3\n",
    "\n",
    "# Verify GPU is available\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úì PyTorch {torch.__version__}\")\n",
    "print(f\"‚úì Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Start - Single Image Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the model { display-mode: \"form\" }\n",
    "# @markdown Choose your model size (larger = more accurate but slower)\n",
    "\n",
    "model_name = \"DA3-LARGE\"  # @param [\"DA3-SMALL\", \"DA3-BASE\", \"DA3-LARGE\", \"DA3-GIANT\"]\n",
    "\n",
    "from depth_anything_3.api import DepthAnything3\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = DepthAnything3.from_pretrained(f\"depth-anything/{model_name}\")\n",
    "model = model.to(device)\n",
    "print(f\"‚úì Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download sample image { display-mode: \"form\" }\n",
    "\n",
    "!wget -q https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/1280px-Camponotus_flavomarginatus_ant.jpg -O sample.jpg\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"sample.jpg\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Image size: {img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run depth estimation { display-mode: \"form\" }\n",
    "\n",
    "# Run inference\n",
    "result = model.inference([\"sample.jpg\"])\n",
    "\n",
    "print(\"‚úì Inference complete!\")\n",
    "print(f\"  Depth shape: {result.depth.shape}\")\n",
    "print(f\"  Confidence shape: {result.conf.shape}\")\n",
    "print(f\"  Extrinsics shape: {result.extrinsics.shape}\")\n",
    "print(f\"  Intrinsics shape: {result.intrinsics.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize depth map { display-mode: \"form\" }\n",
    "\n",
    "import numpy as np\n",
    "from depth_anything_3.utils.io import visualize_depth\n",
    "\n",
    "# Get depth map\n",
    "depth = result.depth[0]  # First (only) image\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(result.processed_images[0])\n",
    "axes[0].set_title(\"Input Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Depth map (colorized)\n",
    "depth_colored = visualize_depth(depth, colormap=\"Spectral\")\n",
    "axes[1].imshow(depth_colored)\n",
    "axes[1].set_title(f\"Depth Map (min={depth.min():.2f}, max={depth.max():.2f})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Outputs\n",
    "\n",
    "The `result` object contains several useful attributes:\n",
    "\n",
    "| Attribute | Shape | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `depth` | `[N, H, W]` | Depth maps (in relative or metric units) |\n",
    "| `conf` | `[N, H, W]` | Confidence maps (0-1) |\n",
    "| `extrinsics` | `[N, 3, 4]` | Camera extrinsics (world-to-camera, OpenCV format) |\n",
    "| `intrinsics` | `[N, 3, 3]` | Camera intrinsics (focal length, principal point) |\n",
    "| `processed_images` | `[N, H, W, 3]` | Resized input images (uint8) |\n",
    "\n",
    "Where `N` is the number of input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Explore camera intrinsics { display-mode: \"form\" }\n",
    "\n",
    "K = result.intrinsics[0]\n",
    "print(\"Camera Intrinsics Matrix (K):\")\n",
    "print(K)\n",
    "print(f\"\\nFocal length (fx, fy): ({K[0,0]:.1f}, {K[1,1]:.1f})\")\n",
    "print(f\"Principal point (cx, cy): ({K[0,2]:.1f}, {K[1,2]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-View 3D Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download multi-view example { display-mode: \"form\" }\n",
    "\n",
    "import os\n",
    "\n",
    "# Download a few views of the same scene\n",
    "os.makedirs(\"multiview\", exist_ok=True)\n",
    "!wget -q https://raw.githubusercontent.com/ByteDance-Seed/Depth-Anything-3/main/assets/examples/SOH/000.png -O multiview/000.png\n",
    "!wget -q https://raw.githubusercontent.com/ByteDance-Seed/Depth-Anything-3/main/assets/examples/SOH/010.png -O multiview/010.png\n",
    "!wget -q https://raw.githubusercontent.com/ByteDance-Seed/Depth-Anything-3/main/assets/examples/SOH/020.png -O multiview/020.png\n",
    "\n",
    "images = sorted([f\"multiview/{f}\" for f in os.listdir(\"multiview\") if f.endswith(\".png\")])\n",
    "print(f\"Loaded {len(images)} images\")\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, img_path in enumerate(images):\n",
    "    axes[i].imshow(Image.open(img_path))\n",
    "    axes[i].set_title(f\"View {i+1}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run multi-view inference { display-mode: \"form\" }\n",
    "\n",
    "result_mv = model.inference(images)\n",
    "\n",
    "print(\"‚úì Multi-view inference complete!\")\n",
    "print(f\"  Depth maps: {result_mv.depth.shape}\")\n",
    "print(f\"  Camera poses: {result_mv.extrinsics.shape}\")\n",
    "\n",
    "# Visualize all depth maps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for i in range(3):\n",
    "    axes[0, i].imshow(result_mv.processed_images[i])\n",
    "    axes[0, i].set_title(f\"View {i+1}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    \n",
    "    depth_viz = visualize_depth(result_mv.depth[i], colormap=\"Spectral\")\n",
    "    axes[1, i].imshow(depth_viz)\n",
    "    axes[1, i].set_title(f\"Depth {i+1}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Compare different model sizes to understand the speed/quality tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compare model sizes { display-mode: \"form\" }\n",
    "# @markdown ‚ö†Ô∏è This will download and run 3 models (~5-10 minutes)\n",
    "\n",
    "import time\n",
    "\n",
    "models_to_compare = [\"DA3-SMALL\", \"DA3-BASE\", \"DA3-LARGE\"]\n",
    "results_comparison = {}\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {model_name}...\")\n",
    "    \n",
    "    # Load model\n",
    "    m = DepthAnything3.from_pretrained(f\"depth-anything/{model_name}\").to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = m.inference([\"sample.jpg\"])\n",
    "    \n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    start = time.perf_counter()\n",
    "    result = m.inference([\"sample.jpg\"])\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    results_comparison[model_name] = {\n",
    "        \"time\": elapsed,\n",
    "        \"depth\": result.depth[0],\n",
    "    }\n",
    "    print(f\"  Inference time: {elapsed*1000:.1f} ms\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del m\n",
    "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Summary:\")\n",
    "for name, data in results_comparison.items():\n",
    "    print(f\"  {name}: {data['time']*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Visualize model comparison { display-mode: \"form\" }\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models_to_compare), figsize=(15, 5))\n",
    "\n",
    "for i, (name, data) in enumerate(results_comparison.items()):\n",
    "    depth_viz = visualize_depth(data[\"depth\"], colormap=\"Spectral\")\n",
    "    axes[i].imshow(depth_viz)\n",
    "    axes[i].set_title(f\"{name}\\n{data['time']*1000:.0f}ms\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Depth Maps by Model Size\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Export to GLB (3D viewable) { display-mode: \"form\" }\n",
    "\n",
    "from depth_anything_3.utils.export import export\n",
    "\n",
    "# Reload model for export\n",
    "model = DepthAnything3.from_pretrained(\"depth-anything/DA3-LARGE\").to(device)\n",
    "result = model.inference(images)  # Multi-view\n",
    "\n",
    "# Export to GLB\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "export(\n",
    "    result,\n",
    "    export_dir=\"output\",\n",
    "    export_format=\"glb\",\n",
    "    conf_thresh_percentile=10,\n",
    "    num_max_points=50000,\n",
    "    show_cameras=True,\n",
    ")\n",
    "\n",
    "print(\"‚úì Exported to output/\")\n",
    "!ls -la output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Download results { display-mode: \"form\" }\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Zip and download\n",
    "!zip -r output.zip output/\n",
    "files.download(\"output.zip\")\n",
    "print(\"\\n‚úì Download started! Open the .glb file in https://gltf-viewer.donmccurdy.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save to Google Drive (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Mount Google Drive { display-mode: \"form\" }\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create output directory\n",
    "drive_output = \"/content/drive/MyDrive/DA3_Results\"\n",
    "os.makedirs(drive_output, exist_ok=True)\n",
    "print(f\"‚úì Output directory: {drive_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save results to Drive { display-mode: \"form\" }\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Copy output to Drive\n",
    "for f in os.listdir(\"output\"):\n",
    "    shutil.copy(f\"output/{f}\", drive_output)\n",
    "    print(f\"  Saved: {f}\")\n",
    "\n",
    "print(f\"\\n‚úì All files saved to Google Drive: {drive_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tips & Best Practices\n",
    "\n",
    "### Model Selection\n",
    "- **DA3-SMALL**: Fast, good for real-time or many images\n",
    "- **DA3-BASE**: Good balance of speed and quality\n",
    "- **DA3-LARGE**: High quality, recommended for most use cases\n",
    "- **DA3-GIANT**: Highest quality, requires more VRAM\n",
    "\n",
    "### Memory Management\n",
    "- Use `batch_inference()` for large image sets\n",
    "- Set `batch_size=\"auto\"` for automatic memory management\n",
    "- Clear cache with `torch.cuda.empty_cache()` between runs\n",
    "\n",
    "### Quality Tips\n",
    "- More views = better 3D reconstruction\n",
    "- Higher confidence threshold = cleaner point cloud\n",
    "- Use `process_res=\"high_res\"` for detailed depth maps\n",
    "\n",
    "---\n",
    "\n",
    "## Credits\n",
    "\n",
    "This notebook uses **Depth Anything 3** by ByteDance:\n",
    "- [Paper](https://arxiv.org/abs/2511.10647)\n",
    "- [Project Page](https://depth-anything-3.github.io)\n",
    "- [Original Repository](https://github.com/ByteDance-Seed/Depth-Anything-3)\n",
    "\n",
    "Optimized fork: [awesome-depth-anything-3](https://github.com/Aedelon/awesome-depth-anything-3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
