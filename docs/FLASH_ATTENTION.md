# Flash Attention Integration

Quick guide to Flash Attention support in Depth Anything 3.

## The Short Story

**PyTorch 2.2+ already includes Flash Attention!** ðŸŽ‰

The built-in `F.scaled_dot_product_attention()` (SDPA) automatically selects the best backend for your hardware:
- **NVIDIA GPU (CUDA)** : Uses Flash Attention v2 natively â†’ Already optimized âœ…
- **Apple Silicon (MPS)** : Uses Metal optimized kernels â†’ Already optimized âœ…
- **CPU** : Uses optimized implementation â†’ Already good enough âœ…

**Optional:** You can install the standalone `flash-attn` package for potentially cutting-edge optimizations on CUDA, but **it's not necessary** for most use cases.

## Do I Need flash-attn Package?

### âœ… You DON'T need it if:
- You have PyTorch â‰¥ 2.2 (Flash Attention is built-in via SDPA)
- You're on Apple Silicon (MPS) â€” not supported anyway
- You're on CPU â€” not applicable
- You want simplicity â€” PyTorch native works great

### ðŸ¤” Consider it if:
- You're on NVIDIA GPU and want bleeding-edge optimizations
- You need the absolute latest Flash Attention v3 features
- You're benchmarking and squeezing every millisecond

## Installation (Optional, CUDA only)

```bash
# Only if you want the standalone flash-attn package
pip install flash-attn --no-build-isolation
```

**Requirements:**
- CUDA Compute Capability â‰¥ 7.5 (Tesla V100+, RTX 20/30/40 series, A100, etc.)
- CUDA toolkit
- PyTorch â‰¥ 2.2
- ninja (usually auto-installed)
- Linux only (compilation required)

**Note:** Does NOT work on macOS or Windows native (use WSL2 on Windows)

## Check Your Setup

### Check if PyTorch SDPA uses Flash Attention

```python
import torch

if torch.cuda.is_available():
    from torch.backends.cuda import flash_sdp_enabled, mem_efficient_sdp_enabled

    print(f"PyTorch version: {torch.__version__}")
    print(f"Flash SDP enabled: {flash_sdp_enabled()}")
    print(f"Memory Efficient SDP enabled: {mem_efficient_sdp_enabled()}")
```

**Expected output on CUDA with PyTorch â‰¥ 2.2:**
```
PyTorch version: 2.5.1+cu121
Flash SDP enabled: True
Memory Efficient SDP enabled: True
```

âœ… If `Flash SDP enabled: True`, **you already have Flash Attention!** No need to install anything.

### Check DA3 Backend Selection

```python
from depth_anything_3.model.dinov2.layers import FLASH_ATTN_AVAILABLE, get_attention_backend

print(f"flash-attn package: {FLASH_ATTN_AVAILABLE}")
print(f"Selected backend: {get_attention_backend()}")
```

**Output examples:**

On NVIDIA GPU with PyTorch 2.2+ (no flash-attn package):
```
flash-attn package: False
Selected backend: sdpa
```
âœ… **This is fine!** SDPA uses Flash Attention internally.

On NVIDIA GPU with flash-attn package installed:
```
flash-attn package: True
Selected backend: flash_attn
```
âœ… Will use standalone flash-attn for potentially extra optimizations.

On Apple Silicon or CPU:
```
flash-attn package: False
Selected backend: sdpa
```
âœ… Uses best available backend for your hardware.

## How It Works

The code **automatically selects the best backend**:

1. **flash_attn** (if installed + NVIDIA GPU) â†’ Fastest âš¡
2. **sdpa** (PyTorch 2.0+ default) â†’ Fast, built-in âœ“
3. **manual** (fallback) â†’ Slow, for debugging

No code changes needed, it just works transparently.

## Override Backend (Advanced)

Force a specific backend with environment variable:

```bash
# Use Flash Attention explicitly
export DA3_ATTENTION_BACKEND=flash_attn
python inference.py

# Force PyTorch SDPA (debug)
export DA3_ATTENTION_BACKEND=sdpa
python inference.py

# Force manual attention (very slow, debugging only)
export DA3_ATTENTION_BACKEND=manual
python inference.py
```

## Benchmark Your Hardware

Run the comprehensive benchmark to see actual performance on your hardware:

```bash
# Standard benchmark (SDPA only)
python benchmarks/flash_attention_benchmark.py

# Detailed benchmark (includes manual attention comparison)
python benchmarks/flash_attention_benchmark.py --detailed
```

### Expected Results

**On NVIDIA GPU (CUDA) with PyTorch 2.2+:**
```
ðŸ“Š HARDWARE CONFIGURATION
  Device Type      : CUDA
  Device Name      : NVIDIA RTX 4090
  Compute Cap.     : 8.9 âœ… Flash Attention supported

  PyTorch SDPA Backends:
    Flash SDP      : âœ… Enabled
    MemEfficient   : âœ… Enabled
    Math SDP       : âœ… Enabled

  âš¡ PyTorch SDPA uses Flash Attention internally!
     (No need for flash-attn package with PyTorch >= 2.2)

ðŸ”¬ MODEL: VITL (dim=1024, heads=16, depth=24)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration            â”‚ sdpa         â”‚ manual       â”‚ Speedup    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Small (392px)            â”‚   0.08 ms    â”‚   0.15 ms    â”‚  1.9x âš¡   â”‚
â”‚ Medium (518px)           â”‚   0.12 ms    â”‚   0.28 ms    â”‚  2.3x âš¡   â”‚
â”‚ Large (742px)            â”‚   0.18 ms    â”‚   0.52 ms    â”‚  2.9x âš¡   â”‚
â”‚ XLarge (1024px)          â”‚   0.25 ms    â”‚   0.89 ms    â”‚  3.6x âš¡   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Flash Attention is ACTIVE via PyTorch SDPA!
   â€¢ 2-4x faster attention vs manual implementation
   â€¢ ~15-25% overall inference speedup
```

**On Apple Silicon (MPS):**
```
ðŸ“Š HARDWARE CONFIGURATION
  Device Type      : MPS
  Device Name      : Apple Silicon
  PyTorch         : 2.9.0

ðŸ”¬ ATTENTION BENCHMARK (per layer, actual results)
Config               SDPA         Manual       Speedup
ViT-L 518px          2.19 ms      1.84 ms      0.8x âš ï¸
ViT-L 1024px         8.65 ms      5.72 ms      0.7x âš ï¸

ðŸ“± Apple Silicon (MPS) Analysis
   â€¢ Flash Attention not available for MPS (CUDA only)
   â€¢ PyTorch SDPA uses Metal Performance Shaders
   âš ï¸  SDPA is SLOWER than manual for large sequences on MPS
   â€¢ This is a known MPS limitation for complex attention ops
   â€¢ For MPS: Manual attention may actually be faster

ðŸ”¬ END-TO-END INFERENCE (1280x720 input, actual results)
Model        Best Config         Performance
da3-small    SDPA               22.9 img/s  (1.6x vs manual)
da3-base     SDPA/Manual        10.6 img/s  (equivalent)
da3-large    SDPA/Manual         3.8 img/s  (equivalent)
da3-giant    Manual              1.6 img/s  (slightly better)

Recommendation for Apple Silicon:
   â€¢ Small models: Use SDPA (default)
   â€¢ Large models: Manual may be better, test with:
     export DA3_ATTENTION_BACKEND=manual
```

**Key Takeaway:** On modern PyTorch (â‰¥2.2), SDPA is already 2-4x faster than manual attention thanks to built-in Flash Attention on CUDA.

## Architecture Notes

### Flash Attention Specifics

- **Format**: Expects (Batch, SeqLen, Heads, HeadDim) internally
- **Dtype**: Converts float32 â†’ bfloat16 for computation (more efficient), converts back
- **Attention Mask**: Limited support
  - âœ… No mask (standard self-attention) â†’ Uses Flash Attention
  - âŒ Custom masks â†’ Falls back to SDPA
  - Our RoPE (2D rotary embeddings) â†’ Applied before, fully compatible

### Supported Configurations

| Model | Backbone | Seq Len | Attention |
|-------|----------|---------|-----------|
| DA3-Small | ViT-S | 196 | 12 layers |
| DA3-Base | ViT-B | 256 | 12 layers |
| DA3-Large | ViT-L | 1024 | 24 layers â­ |
| DA3-Giant | ViT-G | 1369 | 40 layers â­â­ |

Larger models benefit most from Flash Attention (more attention layers).

## Performance Impact

### On Attention Layers
- **SDPA (PyTorch 2.2+)**: 2-4x faster than manual attention
- **flash-attn package**: Potentially 5-10% faster than SDPA on some workloads
- Why: IO-aware algorithm, reduced memory bandwidth, fused operations

### On Full Inference
- Attention = ~15-20% of total inference time
- SDPA speedup = **15-25% overall** (vs manual attention baseline)
- flash-attn package = **+5-10% extra** in best case

Example (DA3-Large on 1024px image, NVIDIA A100):
```
Manual attention:      2.5 seconds
SDPA (PyTorch 2.2+):   2.1 seconds  (16% faster) âœ…
flash-attn package:    2.0 seconds  (20% faster) âœ¨
```

**Recommendation:** Start with PyTorch 2.2+ SDPA (built-in, zero setup). Only add flash-attn package if benchmarks show meaningful gains for your specific workload.

## Troubleshooting

### Q: Flash Attention not detected despite installation?

```python
import flash_attn
print(flash_attn.__version__)
```

If import fails, reinstall:
```bash
pip uninstall flash-attn
pip install flash-attn --no-build-isolation
```

### Q: Getting CUDA errors?

Check CUDA Compute Capability:
```python
import torch
print(torch.cuda.get_device_capability())  # Should be >= (7, 5)
```

If lower, Flash Attention won't work (use SDPA instead).

### Q: On Windows?

Flash Attention requires compilation. Use:
- WSL2 (Windows Subsystem for Linux) with CUDA, OR
- GPU-enabled Colab/cloud notebooks

### Q: On macOS?

Not supported. PyTorch SDPA with Metal optimizations is already good enough.

## Implementation Details

See: `src/depth_anything_3/model/dinov2/layers/attention.py`

Key components:
- `FLASH_ATTN_AVAILABLE`: Detects if flash-attn is installed
- `get_attention_backend()`: Auto-selects best backend
- `Attention._flash_attention()`: Flash Attention implementation
- `Attention._sdpa_attention()`: PyTorch SDPA fallback
- `Attention._manual_attention()`: Classic attention (debug)

All backed by RoPE 2D (rotary position embeddings) support.

## References

- [Flash Attention paper](https://arxiv.org/abs/2205.14135)
- [Flash Attention repo](https://github.com/dao-ailab/flash-attention)
- [PyTorch SDPA docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)